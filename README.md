# WASP Deep Learning for NLP project: Pretraining on structured data

The code for generating my pretraining data is in `empirical-dependencies`. This code is adapted from _Yadav, Himanshu, Husain, Samar and Futrell, Richard. "Do dependency lengths explain constraints on crossing dependencies?" Linguistics Vanguard, vol. 7, no. s3, 2021, pp. 20190070. https://doi.org/10.1515/lingvan-2019-0070_.

The code for generating the pretraining data of Papadimitriou and Jurafsky is in `injecting-structural-hints/formal-corpora`.

The code for finetuning on natural language is in `injecting-structural-hints/finetuning/`.

The underlying training code is in `mistral`.

This repository is mainly intended for looking; if you actually want to run the code, you may want to talk to me.
